{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6dcf162-9bc1-4e6f-99ad-ca93480dd87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/home/lexxsh/miniconda3/envs/lex/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/shared/home/lexxsh/miniconda3/envs/lex/lib/python3.8/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import whisper\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import random\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import resample\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from src.models import (\n",
    "    lcnn,\n",
    "    specrnet,\n",
    "    whisper_specrnet,\n",
    "    rawnet3,\n",
    "    whisper_lcnn,\n",
    "    meso_net,\n",
    "    whisper_meso_net\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6442f5-5855-4af1-bd25-8c3522b96e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paths(data, base_path):\n",
    "    data['path'] = data['path'].apply(lambda p: p.replace('./', f'{base_path}'))\n",
    "    return data\n",
    "\n",
    "def load_and_split_data(csv_file, base_path='../SW/'):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data = update_paths(data, base_path)\n",
    "\n",
    "    # 라벨이 'real'인 데이터를 필터링합니다.\n",
    "    real_data = data[data['label'] == 'real']\n",
    "\n",
    "    # 라벨이 'fake'인 데이터를 필터링합니다.\n",
    "    fake_data = data[data['label'] == 'fake']\n",
    "\n",
    "    # real_data와 fake_data의 길이를 맞추기 위해 최소 길이를 사용합니다.\n",
    "    min_length = min(len(real_data), len(fake_data))\n",
    "\n",
    "    # 각각 real_data와 fake_data에서 min_length만큼 샘플링하여 길이를 맞춥니다.\n",
    "    real_data = real_data.sample(min_length).reset_index(drop=True)\n",
    "    fake_data = fake_data.sample(min_length).reset_index(drop=True)\n",
    "\n",
    "    # 랜덤 인덱스를 생성하여 데이터프레임을 셔플합니다.\n",
    "    shuffled_real_indices = np.random.permutation(min_length)\n",
    "    shuffled_fake_indices = np.random.permutation(min_length)\n",
    "\n",
    "    # 랜덤하게 결합할 데이터프레임을 생성합니다.\n",
    "    real_data_1 = real_data.iloc[shuffled_real_indices].reset_index(drop=True)\n",
    "    real_data_2 = real_data.iloc[shuffled_fake_indices].reset_index(drop=True).add_suffix('.1')\n",
    "\n",
    "    real_real = pd.concat([real_data_1, real_data_2], axis=1)\n",
    "    \n",
    "    fake_data_2 = fake_data.iloc[shuffled_fake_indices].reset_index(drop=True).add_suffix('.1')\n",
    "    real_fake = pd.concat([real_data_1, fake_data_2], axis=1)\n",
    "    \n",
    "    fake_data_1 = fake_data.iloc[shuffled_real_indices].reset_index(drop=True)\n",
    "    fake_fake = pd.concat([fake_data_1, fake_data_2], axis=1)\n",
    "\n",
    "    return real_real, real_fake, fake_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b22930-ff99-481f-8c87-1910fb21341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_real, real_fake, fake_fake = load_and_split_data('../SW/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25bce797-adc0-4130-96df-f396ea49fb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                      path label      id.1  \\\n",
      "0  LAIUZNEC  ../SW/train/LAIUZNEC.ogg  real  HBUZAZXG   \n",
      "1  TQXMDGGR  ../SW/train/TQXMDGGR.ogg  real  UUKHJKCW   \n",
      "2  RYPKYFPA  ../SW/train/RYPKYFPA.ogg  real  NXRMXSXE   \n",
      "3  NPCLVTZZ  ../SW/train/NPCLVTZZ.ogg  real  DFVUEIAC   \n",
      "4  RIJGVOPW  ../SW/train/RIJGVOPW.ogg  real  QWRMCZOT   \n",
      "\n",
      "                     path.1 label.1  \n",
      "0  ../SW/train/HBUZAZXG.ogg    real  \n",
      "1  ../SW/train/UUKHJKCW.ogg    real  \n",
      "2  ../SW/train/NXRMXSXE.ogg    real  \n",
      "3  ../SW/train/DFVUEIAC.ogg    real  \n",
      "4  ../SW/train/QWRMCZOT.ogg    real  \n",
      "         id                      path label      id.1  \\\n",
      "0  LAIUZNEC  ../SW/train/LAIUZNEC.ogg  real  OYLDHFON   \n",
      "1  TQXMDGGR  ../SW/train/TQXMDGGR.ogg  real  NLXQBKQV   \n",
      "2  RYPKYFPA  ../SW/train/RYPKYFPA.ogg  real  LBJYEANR   \n",
      "3  NPCLVTZZ  ../SW/train/NPCLVTZZ.ogg  real  GGXVWDDW   \n",
      "4  RIJGVOPW  ../SW/train/RIJGVOPW.ogg  real  GZTVGYWA   \n",
      "\n",
      "                     path.1 label.1  \n",
      "0  ../SW/train/OYLDHFON.ogg    fake  \n",
      "1  ../SW/train/NLXQBKQV.ogg    fake  \n",
      "2  ../SW/train/LBJYEANR.ogg    fake  \n",
      "3  ../SW/train/GGXVWDDW.ogg    fake  \n",
      "4  ../SW/train/GZTVGYWA.ogg    fake  \n"
     ]
    }
   ],
   "source": [
    "print(real_real.head())\n",
    "print(real_fake.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c27cdeca-46e2-4eb9-a425-2c7c90d93f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_real columns: Index(['id', 'path', 'label', 'id.1', 'path.1', 'label.1'], dtype='object')\n",
      "real_fake columns: Index(['id', 'path', 'label', 'id.1', 'path.1', 'label.1'], dtype='object')\n",
      "fake_fake columns: Index(['id', 'path', 'label', 'id.1', 'path.1', 'label.1'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"real_real columns:\", real_real.columns)\n",
    "print(\"real_fake columns:\", real_fake.columns)\n",
    "print(\"fake_fake columns:\", fake_fake.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c666592c-5744-43d9-85c9-29cf375e021f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_and_split_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     52\u001b[0m     csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../SW/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 실제 CSV 파일 경로\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 48\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(csv_file)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(csv_file):\n\u001b[0;32m---> 48\u001b[0m     real_real, real_fake, fake_fake \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_split_data\u001b[49m(csv_file)\n\u001b[1;32m     49\u001b[0m     create_audio_set(real_real, real_fake, fake_fake)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_and_split_data' is not defined"
     ]
    }
   ],
   "source": [
    "def resample_audio(data, original_rate, target_rate):\n",
    "    number_of_samples = round(len(data) * float(target_rate) / original_rate)\n",
    "    resampled_data = resample(data, number_of_samples)\n",
    "    return resampled_data\n",
    "\n",
    "def concatenate_audios_overlap(file_path1, file_path2, output_path):\n",
    "    data1, samplerate1 = sf.read(file_path1)\n",
    "    data2, samplerate2 = sf.read(file_path2)\n",
    "    if samplerate1 != samplerate2:\n",
    "        if samplerate1 > samplerate2:\n",
    "            data2 = resample_audio(data2, samplerate2, samplerate1)\n",
    "            samplerate2 = samplerate1\n",
    "        else:\n",
    "            data1 = resample_audio(data1, samplerate1, samplerate2)\n",
    "            samplerate1 = samplerate2\n",
    "    if len(data1) > len(data2):\n",
    "        data1 = data1[:len(data2)]\n",
    "    else:\n",
    "        data2 = data2[:len(data1)]\n",
    "    combined = data1 + data2\n",
    "    sf.write(output_path, combined, samplerate1)\n",
    "def create_dir_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def create_audio_set(real_real, real_fake, fake_fake):\n",
    "    create_dir_if_not_exists('combined_audio')\n",
    "    \n",
    "    for i, row in tqdm(real_real.iterrows(), total=real_real.shape[0], desc=\"Processing real-real pairs\"):\n",
    "        file_path1 = row['path']\n",
    "        file_path2 = row['path.1']\n",
    "        output_path = f'combined_audio/real_real/{i}.ogg'\n",
    "        concatenate_audios_overlap(file_path1, file_path2, output_path)\n",
    "\n",
    "    for i, row in tqdm(real_fake.iterrows(), total=real_fake.shape[0], desc=\"Processing real-fake pairs\"):\n",
    "        file_path1 = row['path']\n",
    "        file_path2 = row['path.1']\n",
    "        output_path = f'combined_audio/real_fake/{i}.ogg'\n",
    "        concatenate_audios_overlap(file_path1, file_path2, output_path)\n",
    "\n",
    "    for i, row in tqdm(fake_fake.iterrows(), total=fake_fake.shape[0], desc=\"Processing fake-fake pairs\"):\n",
    "        file_path1 = row['path']\n",
    "        file_path2 = row['path.1']\n",
    "        output_path = f'combined_audio/fake_fake/{i}.ogg'\n",
    "        concatenate_audios_overlap(file_path1, file_path2, output_path)\n",
    "\n",
    "def main(csv_file):\n",
    "    real_real, real_fake, fake_fake = load_and_split_data(csv_file)\n",
    "    create_audio_set(real_real, real_fake, fake_fake)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    csv_file = '../SW/train.csv'  # 실제 CSV 파일 경로\n",
    "    main(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a5110d-a063-4a01-ab92-748429274882",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 16_000\n",
    "APPLY_NORMALIZATION = True\n",
    "APPLY_TRIMMING = True\n",
    "APPLY_PADDING = True\n",
    "FRAMES_NUMBER = 480_000\n",
    "win_length = 400\n",
    "hop_length = 160\n",
    "\n",
    "SOX_SILENCE = [\n",
    "    # Trim silence longer than 0.2s and louder than 1% volume\n",
    "    [\"silence\", \"1\", \"0.2\", \"1%\", \"-1\", \"0.2\", \"1%\"],\n",
    "]\n",
    "\n",
    "SOX_SILENCE = [\n",
    "    [\"silence\", \"1\", \"0.2\", \"1%\", \"-1\", \"0.2\", \"1%\"],\n",
    "]\n",
    "\n",
    "def resample_audio(data, original_rate, target_rate):\n",
    "    number_of_samples = round(len(data) * float(target_rate) / original_rate)\n",
    "    resampled_data = resample(data, number_of_samples)\n",
    "    return resampled_data\n",
    "\n",
    "def apply_preprocessing(waveform, sample_rate):\n",
    "    if sample_rate != SAMPLING_RATE:\n",
    "        waveform, sample_rate = resample_wave(waveform, sample_rate, SAMPLING_RATE)\n",
    "\n",
    "    if waveform.dim() > 1 and waveform.shape[0] > 1:\n",
    "        waveform = waveform[:1, ...]\n",
    "\n",
    "    if APPLY_TRIMMING:\n",
    "        waveform, sample_rate = apply_trim(waveform, sample_rate)\n",
    "\n",
    "    if APPLY_PADDING:\n",
    "        waveform = apply_pad(waveform, FRAMES_NUMBER)\n",
    "    return waveform, sample_rate\n",
    "\n",
    "\n",
    "def resample_wave(waveform, sample_rate, target_sample_rate):\n",
    "    waveform, sample_rate = torchaudio.sox_effects.apply_effects_tensor(\n",
    "        waveform, sample_rate, [[\"rate\", f\"{target_sample_rate}\"]]\n",
    "    )\n",
    "    return waveform, sample_rate\n",
    "\n",
    "\n",
    "def apply_trim(waveform, sample_rate):\n",
    "    waveform_trimmed, sample_rate_trimmed = torchaudio.sox_effects.apply_effects_tensor(\n",
    "        waveform, sample_rate, SOX_SILENCE\n",
    "    )\n",
    "\n",
    "    if waveform_trimmed.size()[1] > 0:\n",
    "        waveform = waveform_trimmed\n",
    "        sample_rate = sample_rate_trimmed\n",
    "    \n",
    "    return waveform, sample_rate\n",
    "\n",
    "\n",
    "def apply_pad(waveform, cut):\n",
    "    waveform = waveform.squeeze(0)\n",
    "    waveform_len = waveform.shape[0]\n",
    "\n",
    "    if waveform_len >= cut:\n",
    "        return waveform[:cut]\n",
    "\n",
    "    num_repeats = int(cut / waveform_len) + 1\n",
    "    padded_waveform = torch.tile(waveform, (1, num_repeats))[:, :cut][0]\n",
    "\n",
    "    return padded_waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8146f46d-bca3-4ec3-bf75-7824e5257545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAudioDataset(Dataset):\n",
    "    def __init__(self, real_real_dir, real_fake_dir, fake_fake_dir, csv_file, transform=None, return_meta=False):\n",
    "        self.real_real_dir = real_real_dir\n",
    "        self.real_fake_dir = real_fake_dir\n",
    "        self.fake_fake_dir = fake_fake_dir\n",
    "        self.samples = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.return_meta = return_meta\n",
    "\n",
    "        self.data_files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # 폴더에서 파일 읽기\n",
    "        self.load_folder_data(self.real_real_dir, [0, 1])\n",
    "        self.load_folder_data(self.real_fake_dir, [1, 1])\n",
    "        self.load_folder_data(self.fake_fake_dir, [1, 0])\n",
    "\n",
    "        # CSV에서 파일 읽기\n",
    "        for _, row in self.samples.iterrows():\n",
    "            self.data_files.append('../SW/' + row['path'])\n",
    "            self.labels.append([1, 0] if row['label'] == 'fake' else [0, 1])\n",
    "\n",
    "    def load_folder_data(self, folder_path, label):\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.ogg'):\n",
    "                self.data_files.append(os.path.join(folder_path, filename))\n",
    "                self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.data_files[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(path, normalize=APPLY_NORMALIZATION)\n",
    "        real_sec_length = len(waveform[0]) / sample_rate\n",
    "\n",
    "        waveform, sample_rate = apply_preprocessing(waveform, sample_rate)\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "        return_data = [waveform, sample_rate, label]\n",
    "        if self.return_meta:\n",
    "            file_id = os.path.basename(path).split('.')[0]\n",
    "            return_data.append((file_id, path, real_sec_length))\n",
    "\n",
    "        return return_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    real_real_dir = './combined_audio/real_real'\n",
    "    real_fake_dir = './combined_audio/real_fake'\n",
    "    fake_fake_dir = './combined_audio/fake_fake'\n",
    "    csv_file = '../SW/train.csv'\n",
    "\n",
    "    dataset = SimpleAudioDataset(real_real_dir, real_fake_dir, fake_fake_dir, csv_file, return_meta=True)\n",
    "    train_size = int(0.8 * len(dataset))  # 80% of data for training\n",
    "    val_size = len(dataset) - train_size  # Remaining 20% for validation\n",
    "    \n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create DataLoader for train and validation sets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de575ac8-b0ad-4999-a115-addcad277656",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'fc1_dim': 1024,\n",
    "    'frontend_algorithm': [\"mfcc\"],\n",
    "    'input_channels': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9f3c04a-70f8-4fe5-8471-580cee95ed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ['mfcc'] frontend\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = meso_net.FrontendMesoInception4(fc1_dim=model_config['fc1_dim'],\n",
    "                       frontend_algorithm=model_config['frontend_algorithm'],\n",
    "                       input_channels=model_config['input_channels'],\n",
    "                       device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f425841d-653d-4f4b-9d76-9ce6009ed3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrontendMesoInception4(\n",
      "  (Incption1_conv1): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (Incption1_conv2_1): Conv2d(1, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (Incption1_conv2_2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (Incption1_conv3_1): Conv2d(1, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (Incption1_conv3_2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "  (Incption1_conv4_1): Conv2d(1, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (Incption1_conv4_2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), bias=False)\n",
      "  (Incption1_bn): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (Incption2_conv1): Conv2d(11, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (Incption2_conv2_1): Conv2d(11, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (Incption2_conv2_2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (Incption2_conv3_1): Conv2d(11, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (Incption2_conv3_2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "  (Incption2_conv4_1): Conv2d(11, 2, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (Incption2_conv4_2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), bias=False)\n",
      "  (Incption2_bn): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1): Conv2d(12, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (maxpooling1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "  (maxpooling2): MaxPool2d(kernel_size=(4, 4), stride=(4, 4), padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe36ae71-cdc1-41d7-8746-47867b70d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, device, epochs, save_path):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # 모델을 학습 모드로 설정\n",
    "        running_loss = 0.0\n",
    "        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} Training\")\n",
    "        \n",
    "        for batch in train_progress:\n",
    "            waveforms, sample_rates, labels, *meta = batch\n",
    "            waveforms = waveforms.to(device)  \n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()  # 옵티마이저 초기화\n",
    "            outputs = model(waveforms)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()  # 역전파 수행\n",
    "            optimizer.step()  # 가중치 업데이트\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            train_progress.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        average_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {average_loss:.4f}\")\n",
    "\n",
    "        # 검증 단계\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # 최고의 모델 저장\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Model saved at {save_path}\")\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    total_val_loss = 0.0\n",
    "    val_progress = tqdm(val_loader, desc=\"Validating\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_progress:\n",
    "            waveforms, sample_rates, labels, *meta = batch\n",
    "            waveforms = waveforms.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(waveforms)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            val_progress.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    average_val_loss = total_val_loss / len(val_loader)\n",
    "    return average_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e521d-3cf7-4d87-b28b-45af9b4957ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 Training: 100%|█████████████████████████████████████████████| 3458/3458 [29:46<00:00,  1.94it/s, loss=0.505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.4902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:15<00:00, 11.49it/s, loss=0.406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Validation Loss: 0.5853\n",
      "Model saved at model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 Training: 100%|██████████████████████████████████████████████| 3458/3458 [29:43<00:00,  1.94it/s, loss=0.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Training Loss: 0.4191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:16<00:00, 11.35it/s, loss=0.386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Validation Loss: 0.5590\n",
      "Model saved at model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 Training: 100%|██████████████████████████████████████████████| 3458/3458 [29:41<00:00,  1.94it/s, loss=0.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Training Loss: 0.4011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:16<00:00, 11.33it/s, loss=0.462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Validation Loss: 0.6995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 Training: 100%|█████████████████████████████████████████████| 3458/3458 [29:42<00:00,  1.94it/s, loss=0.641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Training Loss: 0.3907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:18<00:00, 10.96it/s, loss=0.401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Validation Loss: 0.6225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 Training: 100%|█████████████████████████████████████████████| 3458/3458 [29:42<00:00,  1.94it/s, loss=0.083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Training Loss: 0.3833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:16<00:00, 11.35it/s, loss=0.278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Validation Loss: 0.4453\n",
      "Model saved at model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 Training: 100%|█████████████████████████████████████████████| 3458/3458 [29:43<00:00,  1.94it/s, loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Training Loss: 0.3785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:15<00:00, 11.42it/s, loss=0.273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Validation Loss: 0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 Training: 100%|██████████████████████████████████████████████| 3458/3458 [29:43<00:00,  1.94it/s, loss=0.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Training Loss: 0.3741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:22<00:00, 10.51it/s, loss=0.215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Validation Loss: 0.4422\n",
      "Model saved at model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 Training: 100%|██████████████████████████████████████████████| 3458/3458 [29:43<00:00,  1.94it/s, loss=0.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Training Loss: 0.3712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:16<00:00, 11.35it/s, loss=0.256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Validation Loss: 0.4461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 Training: 100%|█████████████████████████████████████████████| 3458/3458 [29:41<00:00,  1.94it/s, loss=0.293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Training Loss: 0.3677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:16<00:00, 11.28it/s, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Validation Loss: 0.4669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 Training: 100%|████████████████████████████████████████████| 3458/3458 [29:42<00:00,  1.94it/s, loss=0.341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Training Loss: 0.3644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:20<00:00, 10.80it/s, loss=0.246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Validation Loss: 0.4599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 Training: 100%|████████████████████████████████████████████| 3458/3458 [29:42<00:00,  1.94it/s, loss=0.314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Training Loss: 0.3629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:17<00:00, 11.11it/s, loss=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Validation Loss: 0.5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 Training: 100%|████████████████████████████████████████████| 3458/3458 [29:42<00:00,  1.94it/s, loss=0.337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Training Loss: 0.3602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:16<00:00, 11.26it/s, loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Validation Loss: 0.4344\n",
      "Model saved at model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 Training: 100%|████████████████████████████████████████████| 3458/3458 [29:42<00:00,  1.94it/s, loss=0.432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Training Loss: 0.3578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:18<00:00, 11.07it/s, loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Validation Loss: 0.4154\n",
      "Model saved at model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 Training: 100%|████████████████████████████████████████████| 3458/3458 [29:42<00:00,  1.94it/s, loss=0.581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Training Loss: 0.3558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:16<00:00, 11.36it/s, loss=0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Validation Loss: 0.6039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 Training: 100%|█████████████████████████████████████████████| 3458/3458 [29:53<00:00,  1.93it/s, loss=0.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Training Loss: 0.3542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:16<00:00, 11.27it/s, loss=0.208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Validation Loss: 0.4580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 Training: 100%|████████████████████████████████████████████| 3458/3458 [29:43<00:00,  1.94it/s, loss=0.257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Training Loss: 0.3531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████████████████████████████████| 865/865 [01:17<00:00, 11.12it/s, loss=0.252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Validation Loss: 0.5375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 Training: 100%|████████████████████████████████████████████| 3458/3458 [29:41<00:00,  1.94it/s, loss=0.579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Training Loss: 0.3517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating:  97%|███████████████████████████████████████████████████████▍ | 841/865 [01:14<00:02, 11.49it/s, loss=0.971]"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # 이진 분류 손실 함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)  # 옵티마이저 구성\n",
    "epochs = 100  # 에폭 수\n",
    "train(model, train_loader, val_loader, criterion, optimizer, device, epochs, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15824d68-c099-48cc-9042-2847bc5b8477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lexxs",
   "language": "python",
   "name": "lex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
